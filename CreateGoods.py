import numpy as np
import openai
import os
# LlamaIndex ê´€ë ¨ íŒ¨í‚¤ì§€ ì„í¬íŠ¸
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.query_engine import RetrieverQueryEngine

# .env íŒŒì¼ì—ì„œ API í‚¤ ì§ì ‘ ì½ê¸°
with open(".env") as f:
    for line in f:
        key, value = line.strip().split("=")
        os.environ[key] = value  # í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY


# # 0ï¸âƒ£ ì‚¬ìš©ì ê²€ìƒ‰ì–´ ì…ë ¥
# user_query = "ëŒê³ ë˜ ë¬¸ì–‘"

def retrieve_goods(user_query):
    # 1ï¸âƒ£ Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ
    documents = SimpleDirectoryReader("./data").load_data()

    # ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    parser = SentenceSplitter(chunk_size=100, chunk_overlap=20)  # 100 í† í°ì”© ë¶„í• 

    # 4ï¸âƒ£ ë¬¸ì„œë¥¼ Chunk ë‹¨ìœ„ë¡œ ë³€í™˜ í›„ ë²¡í„°í™”
    nodes = parser.get_nodes_from_documents(documents)
        
    index = VectorStoreIndex(nodes, embed_model=embed_model)

    # 5ï¸âƒ£ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (ë¬¸ì„œ ë‚´ë¶€ ë‚´ìš© ê²€ìƒ‰)
    query_engine = RetrieverQueryEngine.from_args(index.as_retriever())

    # 7ï¸âƒ£ ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‚´ìš© ì°¾ê¸°
    retrieved_nodes = query_engine.retrieve(user_query)
    return retrieved_nodes


# ğŸ”¹ GPTë¥¼ ì´ìš©í•´ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
def generate_image_prompt(user_input, retrieved_nodes):
    prompt = f"""
    ì‚¬ìš©ìì˜ ìš”ì²­: {user_input}
    ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©: {retrieved_nodes}
    
    ìœ„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ DALLÂ·E 3ì—ì„œ ì ì ˆí•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.  
    ë‹¨, ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ëŠ” ì‹¤ì œ ìƒí’ˆ ëª©ì—…(mockup)ì²˜ëŸ¼ ë³´ì´ë„ë¡ í•˜ê³ , êµ¿ì¦ˆì— ì´ë¯¸ì§€ê°€ ì ìš©ëœ ëª¨ìŠµì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    response = openai.chat.completions.create(
        model="gpt-4o-mini", # ì§ˆì˜ë‹¹ 0.15ë‹¬ëŸ¬
        messages=[{"role": "system", "content": "ì´ë¯¸ì§€ ìƒì„±ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”."},
                  {"role": "user", "content": prompt}],
        max_tokens=100
    )
    
    return response.choices[0].message.content


# # ğŸ”¹ GPT-4ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± í™•ì¸
# retrieved_nodes = retrieve_goods(user_query)
# optimized_prompt = generate_image_prompt(user_query, retrieved_nodes)
# print(f"ğŸ¨ ìƒì„±ëœ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸: {optimized_prompt}")

# ğŸ”¹ DALLÂ·E 3ë¥¼ ì´ìš©í•´ ì´ë¯¸ì§€ ìƒì„±
def generate_image(prompt):
    response = openai.images.generate(
        model="dall-e-3",
        prompt=prompt,
        quality = "standard",   # í’ˆì§ˆ
        n       = 1,
        size="1024x1024"
    )
    return response.data[0].url


# # ğŸ”¹ DALLÂ·E 3ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ìƒì„± í™•ì¸
# image_url = generate_image(optimized_prompt)
# print(f"ğŸ–¼ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")